{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64cb5662-7d26-43dc-b64b-c67a947ec449",
   "metadata": {},
   "source": [
    "<h1>New</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80859ccb-f7ad-4e69-a915-ff5c3b05609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed minimal CBOW example\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4443a42-c1a3-4249-a5cc-72c538c2987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small corpus\n",
    "sentences = [\"i love deep learning\", \"deep learning is fun\", \"i love fun\", \"i love funny peoples\",\"i love deep learning so much\" ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c9d27a2-e6be-443e-865f-6fe65c99a6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'i': 1, 'love': 2, 'deep': 3, 'learning': 4, 'fun': 5, 'is': 6, 'funny': 7, 'peoples': 8, 'so': 9, 'much': 10}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1   # +1 for padding/index-0 (not used here)\n",
    "print(\"Vocabulary:\", word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eada7fd-b559-41b4-aa94-d9dad3ded6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (54, 1) Y shape: (54, 11)\n"
     ]
    }
   ],
   "source": [
    "# Generate training pairs: for each center word, every surrounding context word -> (context, target)\n",
    "window = 2\n",
    "contexts = []\n",
    "targets = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split()\n",
    "    for idx, center_word in enumerate(tokens):\n",
    "        start = max(idx - window, 0)\n",
    "        end = min(idx + window, len(tokens) - 1)\n",
    "        for i in range(start, end + 1):\n",
    "            if i == idx:\n",
    "                continue\n",
    "            context_word = tokens[i]\n",
    "            contexts.append(word2idx[context_word])   # context token id (input)\n",
    "            targets.append(word2idx[center_word])     # target token id (label)\n",
    "\n",
    "# Convert to arrays\n",
    "X = np.array(contexts)        # shape (N,)\n",
    "y = np.array(targets)         # shape (N,)\n",
    "\n",
    "# IMPORTANT: Embedding expects 2D input (batch, sequence_length)\n",
    "# we used sequence_length=1, so reshape X -> (N,1)\n",
    "X = X.reshape(-1, 1)          # now shape (N,1)\n",
    "\n",
    "# One-hot the targets\n",
    "Y = to_categorical(y, num_classes=vocab_size)  # shape (N, vocab_size)\n",
    "\n",
    "print(\"X shape:\", X.shape, \"Y shape:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df2559fd-228a-41f0-ad06-68e148b9e73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rp332\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rp332\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Embedding for 'deep' (index 3):\n",
      " [-0.48143798  0.62404233 -0.21433535  0.6715141  -0.5054749 ]\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "embedding_dim = 5\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1))\n",
    "# average over sequence axis (here sequence length = 1, so this just squeezes embedding dim)\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1)))   # result shape (batch, embedding_dim)\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train (small epochs is fine for tiny corpus)\n",
    "model.fit(X, Y, epochs=200, batch_size=8, verbose=0)\n",
    "\n",
    "# Inspect embedding for a word\n",
    "word = \"deep\"\n",
    "idx = word2idx[word]\n",
    "emb_matrix = model.get_weights()[0]\n",
    "print(f\"Embedding for '{word}' (index {idx}):\\n\", emb_matrix[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c6bfaeb-77cb-4a3c-ac9d-fa3e3771b856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26844f39880>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train (small epochs is fine for tiny corpus)\n",
    "model.fit(X, Y, epochs=200, batch_size=8, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58b2b3b-80be-4062-b427-82168ba022c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'deep' (index 3):\n",
      " [-0.59979194  0.87316525 -0.24417084  0.78998166 -0.5522912 ]\n"
     ]
    }
   ],
   "source": [
    "# Inspect embedding for a word\n",
    "word = \"deep\"\n",
    "idx = word2idx[word]\n",
    "emb_matrix = model.get_weights()[0]\n",
    "print(f\"Embedding for '{word}' (index {idx}):\\n\", emb_matrix[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b50cf5-7ed8-4b9a-93da-4a1e35038601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context -> Target: love  ->  i\n",
      "Context -> Target: deep  ->  i\n",
      "Context -> Target: i  ->  love\n",
      "Context -> Target: deep  ->  love\n",
      "Context -> Target: learning  ->  love\n",
      "Context -> Target: i  ->  deep\n",
      "Context -> Target: love  ->  deep\n",
      "Context -> Target: learning  ->  deep\n",
      "Context -> Target: love  ->  learning\n",
      "Context -> Target: deep  ->  learning\n"
     ]
    }
   ],
   "source": [
    "# print some (context -> target) readable pairs\n",
    "id2word = {v:k for k,v in word2idx.items()}\n",
    "for i in range(min(10, X.shape[0])):\n",
    "    context_id = X[i,0]\n",
    "    target_id = y[i]\n",
    "    print(f\"Context -> Target: {id2word[context_id]}  ->  {id2word[target_id]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6565ea9-8882-4984-9da4-2ecd2755b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_similar(word):\n",
    "    idx = word2idx.get(word)\n",
    "    if idx is None:\n",
    "        print(\"word not in vocab\")\n",
    "        return\n",
    "    \n",
    "    # X must be 2D with a padded context (context length = 1 here)\n",
    "    x = np.array([[idx]])\n",
    "    \n",
    "    pred = model.predict(x, verbose=0)[0]    # softmax vector\n",
    "    best = np.argmax(pred)\n",
    "    print(\"Input word:\", word)\n",
    "    print(\"Most likely predicted word:\", id2word[best])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1819098a-5162-4b0a-9a7a-fc1f25294bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word: deep\n",
      "Most likely predicted word: learning\n",
      "Input word: learning\n",
      "Most likely predicted word: deep\n"
     ]
    }
   ],
   "source": [
    "predict_similar(\"deep\")\n",
    "predict_similar(\"learning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a4c50-3c75-47f8-97c6-a422cc2e00b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
